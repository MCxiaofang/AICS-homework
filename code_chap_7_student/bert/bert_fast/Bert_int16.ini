[config]
quantization_type = int16
device_mode = clean
int_op_list = FC, BatchMatMul

[model]
input_nodes = input_ids, input_mask, segment_ids
layer_num = 12
seq_length = 128
original_models_path = ./output_dir_squad_v1.1/frozen_model.pb 
output_nodes = unstack
quantization_output_nodes = bert_plugin_op
scope_names = attention/self/query/MatMul, attention/self/key/MatMul,
              attention/self/value/MatMul, attention/self/MatMul,
              attention/self/MatMul_1, attention/output/dense/MatMul,
              intermediate/dense/MatMul, output/dense/MatMul
post_process_name = MatMul
save_model_path = ./output_dir_squad_v1.1/frozen_model_int16.pb

[data]
do_lower_case = False
doc_stride = 128
max_query_length = 64
batch_size = 8
iters = 1
vocab_file = bert/uncased_L-12_H-768_A-12/vocab.txt 
data_list = bert/squad/dev-v1.1.json

